{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# All the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data  import DataLoader,Dataset\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet blocks \n",
    "### Used 6 in my experiment because I worked with 128x128 image size (suggested in the official paper to use 6 for 128x128 and 9 for 256x256 and up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet6block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resnet6block,self).__init__()\n",
    "        self.conv_block = self.make_block()\n",
    "        \n",
    "    def make_block(self):\n",
    "        self.conv_block = []\n",
    "        self.conv_block+=[nn.ReflectionPad2d(1)]\n",
    "        self.conv_block+=[nn.Conv2d(256,256,kernel_size=3,padding=0,bias=True)]\n",
    "        self.conv_block+=[nn.ReflectionPad2d(1)]\n",
    "        self.conv_block+=[nn.Conv2d(256,256,kernel_size=3,padding=0,bias=True)]\n",
    "        \n",
    "        return nn.Sequential(*self.conv_block)\n",
    "    def forward(self,x):\n",
    "        return x + self.conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Generator class of our model\n",
    "### NOTE: You can add 9 resnet blocks by changing the 6 to 9 in the for loop in the __init__() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(3,64,kernel_size=7,stride=1,padding=0,bias=True),\n",
    "                nn.InstanceNorm2d(64),\n",
    "                nn.ReLU(True)]\n",
    "        #downsampling\n",
    "        model+=[\n",
    "               nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(128),\n",
    "                nn.ReLU(True)]\n",
    "        model +=[nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(256),\n",
    "                nn.ReLU(True)]\n",
    "        #6 resnet layers\n",
    "        for i in range(6):\n",
    "            model+= [resnet6block()]\n",
    "        #upsampling layer\n",
    "        model += [nn.Upsample(scale_factor=2),\n",
    "                  nn.ReflectionPad2d(1),\n",
    "                 nn.Conv2d(256,128,kernel_size=3,stride=1,padding=0,bias=True),\n",
    "                 nn.InstanceNorm2d(128),\n",
    "                 nn.ReLU(True)]\n",
    "        model += [nn.Upsample(scale_factor=2),\n",
    "                 nn.ReflectionPad2d(1),\n",
    "                 nn.Conv2d(128,64,kernel_size=3,stride=1,padding=0,bias=True),\n",
    "                 nn.InstanceNorm2d(64),\n",
    "                 nn.ReLU(True)]\n",
    "        model+=[nn.ReflectionPad2d(3),\n",
    "               nn.Conv2d(64,3,kernel_size=7,stride=1,padding=0),\n",
    "               nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "        #return self.model\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Discriminator class\n",
    "\n",
    "### Exactly as suggested in the official paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        model=[nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1,bias=True), \n",
    "              nn.LeakyReLU(0.02,True)]\n",
    "        \n",
    "        model +=[nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(128),\n",
    "                nn.LeakyReLU(0.02,True)]\n",
    "        \n",
    "        model +=[ nn.Conv2d(128,256,kernel_size=4,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(128),\n",
    "                nn.LeakyReLU(0.02,True)]\n",
    "       \n",
    "        model +=[ nn.Conv2d(256,512,kernel_size=4,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(512),\n",
    "                nn.LeakyReLU(0.02,True)]\n",
    "        \n",
    "        model +=[nn.ZeroPad2d((1,0,1,0))]\n",
    "        \n",
    "        model +=[nn.Conv2d(512,1,kernel_size=4,stride=1,padding=1,bias=True)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def  forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights initialising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv')!=-1:\n",
    "        torch.nn.init.normal_(m.weight.data,0.0,0.02)\n",
    "    elif classname.find('BatchNorm2d')!=-1:\n",
    "        torch.nn.init.normal_(m.weight.data,1.0,0.02)\n",
    "        torch.nn.init.constant_(m.bias.data,0.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The buffer store 50 previously generated images\n",
    "### As suggested in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replaybuffer():\n",
    "    def __init__(self,max_size=50):\n",
    "        self.max_size= max_size\n",
    "        self.data =[]\n",
    "    \n",
    "    def push_and_pop(self,data):\n",
    "        to_return =[]\n",
    "        for element in data.data:\n",
    "            element= torch.unsqueeze(element,0)\n",
    "            if len(self.data)<self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if np.random.uniform(0,1)>0.5:\n",
    "                    i = np.random.randint(0,self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.autograd.Variable(torch.cat(to_return))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradually decrease the Learning rates of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.02, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.02, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.02, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.02, inplace=True)\n",
       "    (11): ZeroPad2d(padding=(1, 0, 1, 0), value=0.0)\n",
       "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_A = Generator()\n",
    "generator_B = Generator()\n",
    "discriminator_A = Discriminator()\n",
    "discriminator_B = Discriminator()\n",
    "generator_A.apply(weights_init)\n",
    "generator_B.apply(weights_init)\n",
    "discriminator_A.apply(weights_init)\n",
    "discriminator_B.apply(weights_init)\n",
    "generator_A.cuda()\n",
    "generator_B.cuda()\n",
    "discriminator_A.cuda()\n",
    "discriminator_B.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer with lr = 0.0002 , beta1 = 0.5 and beta2 = 0.999\n",
    "## As suggested in the official paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = torch.optim.Adam(itertools.chain(generator_A.parameters(),generator_B.parameters()),lr = 0.0002, betas=(0.5,0.999),weight_decay=1e-5)\n",
    "optimizerDA = torch.optim.Adam(discriminator_A.parameters(),lr = 0.0002, betas=(0.5,0.999))\n",
    "optimizerDB = torch.optim.Adam(discriminator_B.parameters(),lr=0.0002,betas =(0.5,0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The losses as suggested by the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GANloss = nn.MSELoss()\n",
    "cycle = nn.L1Loss()\n",
    "identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_g = torch.optim.lr_scheduler.LambdaLR(optimizerG,lr_lambda=LambdaLR(200,0,100).step)\n",
    "lr_da = torch.optim.lr_scheduler.LambdaLR(optimizerDA,lr_lambda=LambdaLR(200,0,100).step)\n",
    "lr_db = torch.optim.lr_scheduler.LambdaLR(optimizerDB,lr_lambda=LambdaLR(200,0,100).step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_A_buffer = replaybuffer()\n",
    "fake_B_buffer = replaybuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageA = torchvision.datasets.ImageFolder('/path/to/data/',transform=torchvision.transforms.Compose(\n",
    "[torchvision.transforms.Resize(128),\n",
    " torchvision.transforms.ToTensor(),\n",
    " torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "        \n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageB = torchvision.datasets.ImageFolder('/path/to/data/',transform=torchvision.transforms.Compose(\n",
    "[torchvision.transforms.Resize(128),\n",
    " torchvision.transforms.ToTensor(),\n",
    " torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "        \n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA = DataLoader(imageA,shuffle=True,batch_size = 1, num_workers=8 )\n",
    "dataB = DataLoader(imageB,shuffle=True,batch_size = 1 , num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targets for the patch gan discriminator used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_real = torch.tensor(np.ones((1,1,8,8)),dtype = torch.float32).cuda()\n",
    "target_fake = torch.tensor(np.zeros((1,1,8,8)),dtype= torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    for imgA,imgB in zip(enumerate(dataA),enumerate(dataB)):\n",
    "        generator_A.train()\n",
    "        generator_B.train()\n",
    "        optimizerG.zero_grad()\n",
    "       \n",
    "        setA = imgA[1][0].cuda()\n",
    "        setB = imgB[1][0].cuda()\n",
    "        \n",
    "        fakeA = generator_A(setB)\n",
    "        fakeB = generator_B(setA)\n",
    "        \n",
    "        rec_A = generator_A(fakeB)\n",
    "        rec_B = generator_B(fakeA)\n",
    "        \n",
    "        pred_fake_A = discriminator_A(fakeA)\n",
    "        gla = GANloss(pred_fake_A,target_real)\n",
    "        \n",
    "        pred_fake_B = discriminator_B(fakeB)\n",
    "        glb = GANloss(pred_fake_B,target_real)\n",
    "        \n",
    "        cycle_a = cycle(rec_A,setA)\n",
    "        cycle_b = cycle(rec_B,setB)\n",
    "        \n",
    "        identity_A = generator_A(setA)\n",
    "        identity_B = generator_B(setB)\n",
    "        \n",
    "        identity_loss =(identity(identity_A,setA)+identity(identity_B,setB))/2\n",
    "        \n",
    "        generator_loss = (gla+glb)/2+((cycle_a+cycle_b)/2)*10.0+identity_loss*5.0\n",
    "        \n",
    "        generator_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        #########################################\n",
    "        optimizerDA.requires_grad = True\n",
    "        optimizerDA.zero_grad()\n",
    "        \n",
    "        pred_real_A = discriminator_A(setA)\n",
    "        dar_loss= GANloss(pred_real_A,target_real)\n",
    "        \n",
    "        pred_fake_A = discriminator_A(fake_A_buffer.push_and_pop(fakeA))\n",
    "        daf_loss = GANloss(pred_fake_A,target_fake)\n",
    "        \n",
    "        da_loss = (dar_loss+daf_loss)/2\n",
    "        da_loss.backward()\n",
    "        optimizerDA.step()\n",
    "        \n",
    "        ###########################################\n",
    "        optimizerDB.requires_grad=True\n",
    "        optimizerDB.zero_grad()\n",
    "        pred_real_B = discriminator_B(setB.detach())\n",
    "        dbr_loss = GANloss(pred_real_B,target_real)\n",
    "        \n",
    "        pred_fake_b = discriminator_B(fake_B_buffer.push_and_pop(fakeB))\n",
    "        dbf_loss = GANloss(pred_fake_b,target_fake)\n",
    "        \n",
    "        db_loss = (dbf_loss+dbr_loss)/2\n",
    "        db_loss.backward()\n",
    "        optimizerDB.step()\n",
    "        \n",
    "        ###########################################\n",
    "        \n",
    "        if(imgA[0]%10==0 or imgA[0]==0):\n",
    "            fakeim = torch.cat([setB,fakeA,setA,fakeB],2)\n",
    "            fakeim = fakeim.squeeze(0)\n",
    "            fakeim = fakeim*.5+.5\n",
    "            fakeim = np.transpose(fakeim.cpu().detach())\n",
    "            plt.imshow(fakeim)\n",
    "            plt.show()\n",
    "\n",
    "            print(f'epoch: {i} G:{generator_loss}, D:{db_loss+da_loss}, Cycle:{((cycle_a+cycle_b)/2)*10}')\n",
    "    lr_da.step()\n",
    "    lr_db.step()\n",
    "    lr_g.step()\n",
    "    \n",
    "    torch.save(generator_A.state_dict(),'Path')\n",
    "    torch.save(generator_B.state_dict(),'Path')\n",
    "    torch.save(discriminator_A.state_dict(),'Path')\n",
    "    torch.save(discriminator_B.state_dict(),'Path')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out the official repository https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "\n",
    "\n",
    "# If you'd like a read of the official paper https://arxiv.org/abs/1703.10593 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
